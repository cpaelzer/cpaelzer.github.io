<!DOCTYPE html>
<html lang="en-us">
    <head>
         
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Slow VFIO guest startup</title>
        <style>

    html body {
        font-family: 'Raleway', sans-serif;
        background-color: white;
    }

    :root {
        --accent: green;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://cpaelzer.github.io//css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 


    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

     <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/Bash.min.js"></script>  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/Diff.min.js"></script>  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/shell.min.js"></script>  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/C&#43;&#43;.min.js"></script>  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script> 

    <script>hljs.initHighlightingOnLoad();</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.92.2" />
        
        
        
    </head>

    
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

    <body>
         
        <nav class="navbar navbar-default navbar-fixed-top">

            <div class="container">

                <div class="navbar-header">

                    <a class="navbar-brand visible-xs" href="#">Slow VFIO guest startup</a>

                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>

                </div>

                <div class="collapse navbar-collapse">

                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/blogs/">Tech</a></li>
                            
                                <li><a href="/bbq/">BBQ</a></li>
                            
                                <li><a href="/about">About</a></li>
                            
                                <li><a href="/public_gpg.txt">GPG Key</a></li>
                            
                        </ul>
                    

                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:christian.ehrhardt@canonical.com"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/cpaelzer/"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://launchpad.net/~paelzer"><i class="fa fa-linux"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://stackoverflow.com/users/6361589/christian-ehrhardt"><i class="fa fa-stack-overflow"></i></a></li>
                            
                        </ul>
                    

                </div>

            </div>

        </nav>


<main>

    <div class="item">

    
    
    

    
    

    <h4><a href="/blogs/007-vfio-slow-startup/">Slow VFIO guest startup</a></h4>
    <h5>April 28, 2024</h5>
    
    <a href="https://cpaelzer.github.io/tags/vfio"><kbd class="item-tag">vfio</kbd></a>
    
    <a href="https://cpaelzer.github.io/tags/iommu"><kbd class="item-tag">iommu</kbd></a>
    
    <a href="https://cpaelzer.github.io/tags/qemu"><kbd class="item-tag">qemu</kbd></a>
    
    <a href="https://cpaelzer.github.io/tags/passthrough"><kbd class="item-tag">passthrough</kbd></a>
    

</div>


    <br> <div class="text-justify"><h1 id="slow-guest-startup-caused-by-using-vfio">Slow guest startup caused by using VFIO</h1>
<h2 id="aka-being-lost-without-a-maping">a.k.a being lost without a map(ing)</h2>
<p>Disclaimer: for now this is about the debugging where the time sink is, it mentions ways to mitigate some effects but not a fix.
If later a fix comes up I&rsquo;ll update.</p>
<h3 id="2024-update">2024 Update</h3>
<p>To overcome this Qemu added and evolved pre-allocation quite a bit as you can
often see in the &ldquo;Memory backends&rdquo; of the <a href="https://wiki.qemu.org/ChangeLog/5.0">changelogs</a> of 5.0 and later.
This got further improved, stabilized and in <a href="https://libvirt.org/news.html#v8-2-0-2022-04-01">Libvirt 8.2</a>
got exposure to higher management levels. There as well several fixups landed in
later releases. Look for the details on <a href="https://libvirt.org/formatdomain.html#memory-backing">memory backing configuration</a> to try it.
This isn&rsquo;t fixing all of this, but at the same time the developers of
the other components, like the kernel, haven&rsquo;t been idling either (but there
it is harder for me to point to a single change).
IMHO all that combined today (April 2024) improves the situation a lot and
one can use those features to see it starting much faster in a recent
Ubuntu 24.04 Noble than what I found back in 2019 when analyzing this the
first time. Proper numa config was always important for guests of this size
and also is important for this initialization.</p>
<h3 id="symptom-classic-linear-scaling-at-work">Symptom: classic linear scaling at work</h3>
<p>Starting a KVM guest usually is in the range of a few seconds. Obviously the guest then needs time to boot up, but the preparation phase is rather short.
This isn&rsquo;t true if you use VFIO passthrough. If you do so the memory needs to be allocated, pinned and mapped to the iommu before the guest can really start.
These actions are known to be quite slow/expensive, but there are a few things that make this worse.</p>
<p>The first time I used passthrough guests had the size of probably a half to a few GiB.
But these days guests can grow up to terabytes as in <a href="https://bugs.launchpad.net/ubuntu/+source/qemu/+bug/1838575">bug 1838575</a> when it was reported to me.
I wanted to make sure I really know where the bottlenecks are.</p>
<p>It turns out it all was just <a href="https://en.wikipedia.org/wiki/Big_O_notation">linear scaling of O(n)</a>.
The kernel task that allocates and maps the memory is single threaded, so no matter how big your nice CPU count grows 1TiB is ~1000 time slower than the 1GiB guest of the old days.</p>
<p>Initially it wasn&rsquo;t clear if the guest preparation, the bootloader or the boot inside the guest consumed the time.
Therefore I tracked time in three sections prep (until the console connects) / bootloader (until you see the kernel to initialize) / boot-up (until ready for login).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">| Guest / Time (sec)         | Prep | bootloader | boot-up |
| 512MB 1CPU no-VFIO         |    4 |         12 |     17  |
| 1.2TB 1CPU no-VFIO         |    6 |         21 |     16  |
| 1.2TB 1CPU    VFIO         |  253 |         20 |     18  |
</code></pre></div><p>So the extra time consumed clearly was in the preparation stage when qemu/kvm sets up things
Running <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf</a> along that showed work was mostly in the memory management, in the setup I had around transparent huge page handling.</p>
<pre tabindex="0"><code>  73.91% [kernel] [k] clear_page_erms
   9.53% [kernel] [k] clear_huge_page
   1.65% [kernel] [k] follow_trans_huge_pmd
</code></pre><h3 id="how-to-make-it-worse">How to make it worse</h3>
<p>In the debugging I found that a mid sized 128GiB guest might actually take from a few seconds or up to 10 minutes to start up.
I later realized this was due to fragmentation of the memory which made it need more allocation calls, more locking to get these resources and finally more calls to map things in the <a href="https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit">IOMMU</a>.
We will come later to the details of this, but on top of the linear scaling for raw guest size this could add up to a factor of <code>x15</code> in he experiments that I have run.</p>
<p>That way the 1.x TiB guest can take more than half an hour to start which is over patience limit of even the most relaxed admin and it will have shot it dead before that happens.</p>
<h3 id="immediate-workarounds-that-come-to-mind">Immediate workarounds that come to mind</h3>
<p>If you did not come here for a nice debugging story, but only want this hilarious long guest start time to go down then I&rsquo;d strongly recommend to use huge pages.
Especially with guest sizes &gt;128GiB in my humble opinion 1G <a href="https://libvirt.org/formatdomain.html#elementsMemoryBacking">huge pages</a> pay off huge.
This is due to multiple reasons in this particular case:</p>
<ul>
<li>more memory per page = less mappings</li>
<li>huge pages are only consumed explicitly, which usually means never unless you do so. Due to that:
<ul>
<li>the system more easily finds free pages</li>
<li>almost no fragmentation</li>
</ul>
</li>
</ul>
<p>To be clear, the initialization speed using 1G huge pages still is linear to the guest size.
But you can save an order of magnitude and to a huge extend avoid the further delay amplification by the fragmentation.</p>
<p>Extending the table above</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">| Guest / Time (sec)         | Prep | bootloader | boot-up |
| 512MB 1CPU no-VFIO         |    4 |         12 |     17  |
| 1.2TB 1CPU no-VFIO         |    6 |         21 |     16  |
| 1.2TB 1CPU    VFIO         |  253 |         20 |     18  |
| 1.2TB 1CPU    VFIO noTHP   |  476 |         31 |     20  |
| 1.2TB 1CPU    VFIO 1G-HP   |   59 |         21 |     18  |
</code></pre></div><p>So hugepages help a lot in this case, but all of you who already set up huge pages know it.
Pre-allocating and distributing them among guest is admin/management effort that has its costs in other places.</p>
<h3 id="check-where-time-is-lost---part-i-userspace">Check where time is lost - part I: Userspace</h3>
<p>The obvious next step is to wonder where so much time is lost to optimize it.
To be without pre-judgment to the kernel lets start in Userspace.</p>
<p>Old, simple but still helpful is <code>strace</code> and it clearly pointed to the VFIO ioctl.
Here a case with a ~55 second hang on this call for a 128GiB guest:</p>
<pre tabindex="0"><code>[...]
0.000041 openat(AT_FDCWD, &quot;/dev/vfio/vfio&quot;, O_RDWR|O_CLOEXEC) = 32 &lt;0.000019&gt;
[...]
0.000058 ioctl(32, VFIO_DEVICE_PCI_HOT_RESET or VFIO_IOMMU_MAP_DMA, 0x7ffd774f7f20) = 0 &lt;0.000037&gt;
0.000060 ioctl(32, VFIO_DEVICE_PCI_HOT_RESET or VFIO_IOMMU_MAP_DMA, 0x7ffd774f7f20) = 0 &lt;0.334458&gt;
0.334593 ioctl(32, VFIO_DEVICE_PCI_HOT_RESET or VFIO_IOMMU_MAP_DMA, 0x7ffd774f7f20) = 0 &lt;0.000055&gt;
0.000088 ioctl(32, VFIO_DEVICE_PCI_HOT_RESET or VFIO_IOMMU_MAP_DMA &lt;unfinished ...&gt;
[...]
45.903415 &lt;... ioctl resumed&gt; , 0x7ffd774f7f20) = 0 &lt;55.558237&gt;
</code></pre><p>To be thorough I checked if this was special to my machine, but it affected the new/huge boxes of two major x86 chip manufacturers.
And to be fully complete I checked non x86 on a ppc64 box which was still very similar.</p>
<pre tabindex="0"><code>0.000037 ioctl(17, VFIO_SET_IOMMU, 0x7) = 0 &lt;0.000039&gt;
0.000070 ioctl(17, VFIO_IOMMU_SPAPR_REGISTER_MEMORY &lt;unfinished ...&gt;
[...]
276.894553 &lt;... ioctl resumed&gt; , 0x7fffe3fd6b70) = 0 &lt;286.974436&gt;
</code></pre><h3 id="check-where-time-is-lost---part-ii-which-ioctl-exactly">Check where time is lost - part II: which ioctl exactly</h3>
<p>As you see in  the former section there were multiple such ioctl calls. I wanted to see what made the slow one special.
Fortunately <code>gdb</code> can easily grab ioctls with <code>catch syscall 16</code>.</p>
<p>After a bit of checking for the arguments I came up with this simple <code>qemu</code> and <code>gdb</code> setup that quickly got me directly <em>in front</em> of the slow ioctl.
First I freed the device and started gdb:</p>
<pre tabindex="0"><code>$ virsh nodedev-detach pci_0000_21_00_1 --driver vfio
$ gdb /usr/bin/qemu-system-x86_64
</code></pre><p>And then in gdb we just wait for the right size to come by (my 128GiB example).
From there we catch the next ioctl which always is the right one:</p>
<pre tabindex="0"><code>(gdb) b vfio_dma_map
(gdb) command 1
Type commands for breakpoint(s) 1, one per line.
End with a line saying just &quot;end&quot;.
&gt;silent
&gt;if size != 134217728000
 &gt;cont
 &gt;end
&gt;end
(gdb) run -m 131072 -smp 1 -no-user-config -device vfio-pci,host=21:00.1,id=hostdev0,bus=pci.0,addr=0x7 -enable-kvm
(gdb) catch syscall 16
(gdb) c
</code></pre><p>A backtrace from here looks like this</p>
<pre tabindex="0"><code>(gdb) bt
#0  in ioctl () at ../sysdeps/unix/syscall-template.S:78
#1  in vfio_dma_map (container=0x555557608430, iova=4294967296, size=134217728000,
                     vaddr=0x7fe01fe00000, readonly=false)
#2  in vfio_listener_region_add (listener=0x555557608440, section=0x7fffffffcad0)
#3  in listener_add_address_space (listener=0x555557608440, as=0x5555565ca5a0)
#4  in memory_listener_register (listener=0x555557608440, as=0x5555565ca5a0)
#5  in vfio_connect_container (group=0x5555576083b0, as=0x5555565ca5a0,
                               errp=0x7fffffffdda8)
#6  in vfio_get_group (groupid=45, as=0x5555565ca5a0, errp=0x7fffffffdda8)
#7  in vfio_realize (pdev=0x555557600570, errp=0x7fffffffdda8)
#8  in pci_qdev_realize (qdev=0x555557600570, errp=0x7fffffffde20)
#9  in device_set_realized (obj=0x555557600570, value=true, errp=0x7fffffffdff0)
[...]
#17 in main (argc=14, argv=0x7fffffffe3d8, envp=0x7fffffffe450) at vl.c:4387
</code></pre><p>With that we were set up checking what happens in the kernel right at this call.</p>
<h3 id="check-where-time-is-lost---part-iii-kernel-tracing">Check where time is lost - part III: Kernel tracing</h3>
<p>From perf we knew it must be in some loops for memory management, but I wanted to know more which higher level calls the time was lost.
Eventually I realized the best for my checks would be:</p>
<pre tabindex="0"><code>$ sudo trace-cmd record -p function_graph -l vfio_pin_pages_remote -l vfio_iommu_map
</code></pre><p>I ran that twice, once with a 12 second guest start right after boot (almost no fragmentation).
And then restarting the same guest which took ~175 seconds.</p>
<p>I generally got long repeating sequences like these:</p>
<pre tabindex="0"><code>funcgraph_entry:      ! 428.925 us |        vfio_pin_pages_remote();
funcgraph_entry:        0.771 us   |        vfio_iommu_map();
funcgraph_entry:      ! 426.510 us |        vfio_pin_pages_remote();
[...]
</code></pre><p>Comparing the slow and fast cases I found that the slow case had faster but more of these sequences.
The assumption was that fragmentation might cause this and due to more contention in the memory management to find free consecutive areas and more calls to pin and map these it would take more time.</p>
<h3 id="check-where-time-is-lost---part-iv-systemtap">Check where time is lost - part IV: systemtap</h3>
<p>I wanted to check the count and size of the <code>vfio_pin_pages_remote</code> and <code>vfio_iommu_map</code> activities.
With a systemtap script like this we get all we need:</p>
<pre tabindex="0"><code>probe module(&quot;vfio_iommu_type1&quot;).function(&quot;vfio_iommu_type1_ioctl&quot;) {
    printf(&quot;New vfio_iommu_type1_ioctl\n&quot;);
    start_stopwatch(&quot;vfioioctl&quot;);
}
probe module(&quot;vfio_iommu_type1&quot;).function(&quot;vfio_iommu_type1_ioctl&quot;).return {
    timer=read_stopwatch_ns(&quot;vfioioctl&quot;)
    printf(&quot;Completed vfio_iommu_type1_ioctl: %d\n&quot;, timer);
    stop_stopwatch(&quot;vfioioctl&quot;);
}
probe module(&quot;vfio_iommu_type1&quot;).function(&quot;vfio_pin_pages_remote&quot;) {
    timer=read_stopwatch_ns(&quot;vfioioctl&quot;)
    printf(&quot;%ld: %s\n&quot;, timer, $$parms);
}
</code></pre><p>The stopwatch is like a monotonic &ldquo;add cpu time consumption&rdquo; clock for vfio_iommu_type1_ioctl.
The overall amount of calls we can directly get from the entries and the most important bit was the npage argument that lets us know how big a chunk of consecutive pages it found.
Thereby the trace contained all the information we wanted.</p>
<pre tabindex="0"><code>1154838: dma=0xffff8a65b28e7300 vaddr=0x7fe0a8400000 npage=0x1f3fa00 pfn_base=0xffffac19160a3d48 limit=0x4000
1764663: dma=0xffff8a65b28e7300 vaddr=0x7fe0a8800000 npage=0x1f3f600 pfn_base=0xffffac19160a3d48 limit=0x4000
2375581: dma=0xffff8a65b28e7300 vaddr=0x7fe0a8c00000 npage=0x1f3f200 pfn_base=0xffffac19160a3d48 limit=0x4000
2980276: dma=0xffff8a65b28e7300 vaddr=0x7fe0a9000000 npage=0x1f3ee00 pfn_base=0xffffac19160a3d48 limit=0x4000
3590913: dma=0xffff8a65b28e7300 vaddr=0x7fe0a9400000 npage=0x1f3ea00 pfn_base=0xffffac19160a3d48 limit=0x4000
4200148: dma=0xffff8a65b28e7300 vaddr=0x7fe0a9800000 npage=0x1f3e600 pfn_base=0xffffac19160a3d48 limit=0x4000
</code></pre><p>In general both logs begin to pin and map with some rather large allocations.
But then they drop off to smaller sizes. That is the <a href="https://github.com/torvalds/linux/blob/master/drivers/vfio/vfio_iommu_type1.c#L1003">loop in vfio_pin_map_dma</a> that does that.</p>
<p>With that I was able to compare the fast and slow cases (due to fragmentation).
And it confirmed the theory. Not only - despite all trace overhead - did it still take almost twice as long.
Further it confirmed that the slow case had more and smaller calls.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">case / log(10) of size | 3   | 4    | 5     | 6      | 7     | 8     |
Fast                   | 108 | 1293 | 12133 | 113330 | 27794 | 1119  |
Slow                   | 194 | 1738 | 17375 | 143411 |    55 |    3  |
</code></pre></div><p>This shows for the traced duration that the fast case has a few much larger
calls which the slow case had to make up with much more smaller calls.</p>
<p>P.S. I had no lost events reported, but I assume I have aborted the slow case too early expecting another few thousands of &lt;=6 allocations to come in.
But since I only compared relative behavior and changes over time that should still be valid data IMHO.</p>
<h3 id="outlook---how-to-break-that-linear-scaling">Outlook - how to break that linear scaling</h3>
<p>As mentioned there are several flat improvements possible by the use of huge pages.
But as the time goes on very large guests will become more and more normal.
And to speed up those growing sizes at some point we need to scale better than O(n).</p>
<p>The obvious thought is to do that in multiple threads, but as this is already lock heavy the benefits might be negligible.
Also iommu&rsquo;s are known to be full of quirks like needing the mappings in order or even in a single call - furthermore each of them tends to be slightly different so a solution for one might fail on the other.
We&rsquo;d need to find a way to divide and conquer this keeping these constraints in mind - at least it is only really an issue in very huge sizes so we don&rsquo;t need to split into small chunks (where locks are more of a problem).</p>
<p>After the analysis above I knew in which area to look and - as usual as - soon as you know that you find others with the same issues.
It seems kernel development here is already ongoing, but might be stalled as the last update is from late 2018.</p>
<ul>
<li><a href="https://lwn.net/Articles/770826/">Series implementing some related improvements via ktask</a></li>
<li><a href="https://lwn.net/Articles/771169/">LWN Article about this series</a></li>
<li><a href="https://www.redhat.com/archives/vfio-users/2018-April/msg00020.html">Similar issue reported on memory hot add with vfio</a></li>
<li><a href="https://linuxplumbersconf.org/event/2/contributions/159/attachments/19/146/ktask_lpc_2018_daniel_jordan.pdf">Plumbers presentation on ktask</a></li>
<li><a href="http://lkml.iu.edu/hypermail/linux/kernel/1811.0/03361.html">particular change interesting for my case</a></li>
<li><a href="https://blogs.oracle.com/linux/making-kernel-tasks-faster-with-ktask,-an-update">Blog post about ktask</a></li>
</ul>
</div>

    
    

    

    

</main>

        <footer>

            <p class="copyright text-muted">© 2018-2020 Christian Ehrhardt - Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a></p>

        </footer>
       
    </body>

</html>

